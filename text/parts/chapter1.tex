\chapter{Обзор предметной области}
\label{chapter_review}

\section{Эволюционные алгоритмы}
Эволюционный алгоритм~--- это метод решения задач оптимизации. Данный подход основан на идеях, заимствованных из биологической эволюции: естественный отбор, мутация, скрещивания и наследование признаков. Каждая итерация алгоритма характеризуется набором особей, называемым поколением. На множестве особей вводят функции приспособленности, чтобы количественно оценивать, насколько заданная особь близка к верному решению. При помощи оператора скрещивания (кроссовера) по двум особям генерируется особь для следующего поколения. Оператор мутации вносит малые случайные изменения особи. Начальное поколение обычно формируется случайным образом. При выборе особей для создания нового поколения наиболее приспособленные имеют больше шансов. Общая схема эволюционного алгоритма представлена на листинге~\ref{ea_scheme}.

\begin{algorithm}[h!]
\caption{Общая схема эволюционного алгоритма}
\label{ea_scheme}
\begin{algorithmic}[1]
  \STATE {Создать начальное поколение}
  \STATE {Вычислить значение функции приспособленности для каждой особи}
  \WHILE {(условие останова эволюционного алгоритма не выполнено)}
    \STATE {Выбирается подмножество особей текущего поколения}
    \STATE {Применяя операторы мутации и кроссовера к выбранным особям, генерируются новые}
    \STATE {Вычисляется значение функции приспособленности для сгенерированных особей}    
    \STATE {Формируется новое поколение, заменяя новыми особями наименее приспособленных}
  \ENDWHILE  
\end{algorithmic}
\end{algorithm}

В качестве критерия останова часто используют следующие условия:
\begin{itemize}
 \item найдено верное решение;
 \item достигнуто заданное количество поколений;
 \item превышено заданное время работы;
 \item превышено заданное число вызовов функции приспособленности;
 \item за заданное число поколений не произошло улучшение.
\end{itemize}

Эволюционные алгоритмы применяются для решения задач, к которым не применимы традиционные методы оптимизации. Стоит заметить, что эффективность работы эволюционного алгоритма сильно зависит от выбора его параметров, таких как вероятность мутации, вероятность скрещивания, размер популяции и прочие.

\section{Обучение с подкреплением}
\label{rl}
Алгоритмы обучения с подкреплением часто используется для выбора стратегий в интерактивной среде. Большинство таких алгоритмов не требуют заранее подобранных тестовых примеров, так как их обучение происходит одновременно с применением накопленного опыта.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{rl-scheme.png}
    \caption{Схема алгоритма обучения с подкреплением.}
    \label{rl_scheme}
\end{figure}

Принцип работы алгоритма обучения с подкреплением представлен на схеме~\ref{rl_scheme}. Среда находится в некотором состоянии, которое имеет некоторый набор действий. Агент воздействует на среду, выбирая одно из возможных действий и применяя его к среде. В следствие этого среда может перейти в новое состояние. За выбранное действие агент получает награду. Награда выражается вещественнозначным числом. Награда может быть отрицательна в случае штрафа. Задачей агента является максимизация суммарной награды. Действие, выбранное агентом, определяет не только полученную награду, но и состояние, в которое перейдет среда после его применения.

Задачу обучения с подкреплением в большинстве случаев можно описать как \textit{марковский процесс принятия решений}. Для этого необходимо определить:

\begin{itemize}
    \item дискретное множество состояний среды \textit{S};
    \item дискретное множество действий агента \textit{A};
    \item функцию награды $R : S \times A \rightarrow \mathbb{R}$;
    \item функцию переходов $T : S \times A \times S \rightarrow \mathbb{R}$ При этом $T(s, a, s')$ определяет вероятность перехода из состояния $s$ в состояние $s'$ после применения действия $a$.
\end{itemize}

Выделяют класс алгоритмов, которые используют функцию награды \textit{R} и функцию переходов \textit{T} для определения стратегии поведения. В частности, возможны стратегии в которых алгоритм будет получать незначительную награду в течение некоторого времени, чтобы достичь некоторого состояния среды, которому соответствует большая ожидаемая награда. В рамках данной работы такие алгоритмы не рассматривались.

\subsection{Q-обучение}

Алгоритм \textit{Q}-обучения относится к классу алгоритмов обучения с подкреплением не строящих модель среды. Псевдокод алгоритма представлен на листинге~\ref{q_learning}. Во время работы алгоритма аппроксимируется функция полезности $Q : S \times A \rightarrow \mathbb{R}$, которая описывает ожидаемую награду за действие \textit{a} в состоянии \textit{s}. Для расчета значений \textit{Q} обычно используют \textit{TD}-обучение(temporal difference learning). При этом значения \textit{Q} изменяются по формуле $Q(s, a) = Q(s, a) + \alpha (\gamma Q(s', a') - Q(s, a))$, где $\alpha$ -- скорость обучения, $\gamma$ -- дисконтный фактор.

Выбор действия определяется стратегией исследования среды. Одна из самых простых стратегий - \textit{жадная} заключается в том, чтобы выбирать действие, за которое самое большое ожидаемое вознаграждение, т.е. $\argmax\limits_a{\{Q(s, a)\}}$. Однако в таком случае агент склонен выбирать локально максимальное значение награды, недостаточно обследовав среду. Для улучшения жадной стратегии можно выбирать с вероятностью $\epsilon$ случайное действие, иначе -- действие с максимальной ожидаемой наградой. Такая стратегия называется \textit{$\epsilon$-жадной}. При этом значение $\epsilon$ может меняться во время работы алгоритма, что позволяет перейти от исследования среды к применению накопленного опыта.

\begin{algorithm}[h!]
    \caption{Алгоритм Q-обучения с $\varepsilon$-жадной стратегией исследования среды}
    \label{q_learning}
    \begin{algorithmic}[1]
    \REQUIRE  
        $\varepsilon$ --- вероятность выбора случайного действия;
        $\alpha$ --- скорость обучения;
        $\gamma$ --- дисконтный фактор.
    \STATE {Инициализировать $Q(s, a)$ для всех $s \in S$, $a \in A$}
    \WHILE{{(не достигнуто условие останова)}}
        \STATE {Получить состояние среды $s$}
        \STATE $p \gets ${ случайное вещественное число} $\in [0, 1]$
        \IF {($p \leq \varepsilon$)}
            \STATE $a \gets \arg \max_{a}{Q(s,a)}$
        \ELSE 
            \STATE $a \gets$ { случайное действие } $\in A$
        \ENDIF
        \STATE {Применить действие $a$ к среде}
        \STATE {Получить от среды награду $r$ и состояние $s'$}
        \STATE $Q(s,a) \gets Q(s,a) + \alpha(r + \gamma \max_{a'}{Q(s',a') - Q(s, a))}$
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}

\section{Обзор существующих методов настройки параметров ЭА}

Формально имеется набор $\{v_1, ..., v_n\}$ из $n$ параметров ЭА, каждый из которых может принимать $\{v_{i1}, .., v_{im}\}$ значения. Это могут быть как дискретные значения, так и интервалы значений. Целью алгоритма является выбор таких значений параметров $v_i$, чтобы повысить эффективность ЭА.

Большинство методов адаптивной настройки параметров ЭА можно отнести к классу сопоставителей вероятностей (probability matching techniques), в которых вероятность выбора значения параметра пропорциональна его качеству.

Тем не менее существуют методы настройки параметров ЭА с помощью обучения с подкреплением. 

\subsection{Настройка параметров ЭА как задача для обучения с подкреплением}

При сведении задачи выбора параметров эволюционного алгоритма к обучению с подкреплением в качестве среды выступает эволюционный алгоритм. Агент совершает действие -- устанавливает значения настраиваемых параметров, таких как вероятность мутации или кроссовера. Затем среда переходит в новое состояние, генерируя следующее поколение особей. Награда, возвращаемая агенту средой, является некоторой функцией от значений оптимизируемой функции, вычисленных на особях текущего и предыдущего. Зачастую состояния среды вовсе не выделяют. Считается, что среда находится в одном состоянии.

Значения параметров ЭА обычно ограничены некоторым интервалом значений. Таким образом, чтобы установить значения параметров ЭА, агент дожнен выбрать некоторое значение из непрерывного диапазона. Обычно эту задачу дискретизируют, разделяя диапазон допустимых значений параметра на подинтервалы. Каждый из подинтервалов является допустимым действием для агента. Выбрав действие, агент в качестве значения параметра устанавливает случайное значение из соответствующего подинтервала. Разбиение на интервалы можно делать априорно, т.е. разбиение диапазона значений происходит до запуска алгоритма и не менятся в процессе его работы. Однако для некоторых методов адаптивной настройки параметров было показано, что изменение разбиения во время работы способствует улучшению работы алгоритма. В частности, значение параметра можно подобрать тем точнее, чем меньше шаг разбиения. В то же время это усложняет задачу выбора оптимального подинтервала. К сожалению,  существующие методы настройки параметров ЭА с помощью обучения с подкреплением используют априорное разбиение.

\subsection{Метод, предложенный Karafotias}
\label{karafotias}

На конверенции \textit{Gecco 2014} предложен метод подбора параметров ЭА на основе обучения с подкреплением, в котором множество состояний формировалось во время работы алгоритма, а мнодество действий задается до запуска алгоритма.

Множество состояний среды строилось на основе модели среды \textit{UTree}\ref{utree}. В качестве наблюдаемых характеристик среды для построения модели использовались:

\begin{itemize}
    \item генетическое разнообразие;
    \item разнообразие значений функции приспособленности (среднеквадратичное отклонение);
    \item стагнация (число итераций без улучшения функции приспособленности);
    \item прирост значения функции приспособленности.
\end{itemize}

При определения необходимости разделения состояния среды использовался критерий типа Колмогорова-Смирнова\ref{ks_criteria}.

В качестве алгоритма обучения с подкреплением использовалось \textit{Q}-обучение с $\epsilon$-жадной стратегией исследования среды.

Функции награды для ЭА, максимизирующего функцию приспособленности, задается формулой~(\ref{reward}), где $f_t$ -- значение функции приспособленности ЭА на \textit{t}-ой итерации.
\begin{equation}
\label{reward}
R = c(\frac{f_{t+1}}{f_t} - 1)
\end{equation}
 Для ЭА, которые не ухудшают лудшее известное решение, функция награды неотрицательна. Стоит отметить, что значение функции приспособленности зачастую остается неизменным несколько итераций подряд. В самом деле, чтобы награда была положительная, необходимо улучшить лучшую известную особь. Чтобы замедлить скорость обучения при нулевой награде, припересчет ожидаемой награды \textit{Q} менялся соответствующий коэффициент. А именно:
\begin{align*}
Q(s,a) &= Q(s,a) + \alpha(r)(r + \gamma \max_{a'}{Q(s',a') - Q(s, a))} \\
\alpha(r) &= \begin{cases}\alpha, при r > 0 \\ \alpha_0, иначе \end{cases}
\end{align*}
Стоит отметить, что $\alpha_0 \ll \alpha$.

Диапазон допустимых значений каждого из \textit{k} настраеваемых параметров делится на $m_i$ частей до начала работы алгоритма. Множества допустимых действий агента является декартовым произведением множеств действий, соответствующих некоторому подинтервалу значений каждого из параметров. Соответственно число допустимых действий агента в каждом состоянии равно $\prod\limits_{i = 1}^k{m_i}$. Таким образом выбор значений параметров происходит совместно.

% выбор точки разбиения

\subsubsection{Модель среды \textit{UTree}}
\label{utree}
Эффективность применения алгоритмов обучения с подкреплением экспоненциально уменьшается с увеличением числа возможных состояний среды. Однако в большинстве задач не все из них являются существенными. Одним из подходов, позволяющих уменьшить размерность множества состояний среды является объединение нескольких несущественных состояний. Таким образом множество состояний может быть разбито на несколько значимых состояний. 

Одним из алгоритмов объединения состояний является алгоритм \textit{UTree}. В алгоритме \textit{UTree} строится дерево, листьями которого являются полученные в результате объединения состояния. Также существует вариант алгоритма \textit{UTree}, применимый в случае непрерывного множества состояний. Отличие от алгоритма \textit{UTree} для дискретного случая заключается в том, что он не требует изначального выделения состояний среды. Состояния среды выделяются автоматически в ходе работы алгоритма \textit{UTree}.

Схема работы алгоритма представлена на листинге~\ref{utree_scheme}. Состояния среды выделяются на основании наблюдаемых параметров среды. По своей структуре алгоритм \textit{UTree} представляет собой дерево решений в узлах которого стоят условия на параметры среды.Каждому листу соответствует состояние s среды алгоритма обучения с подкреплением, ожидаемое значение награды в котором обозначается как \textit{V(s)}, где $V(s) = \max \limits_a Q(s,a)$. Изначально в дереве существует лишь один лист, и таким образом у среды есть единственное возможное состояние \textit{s}, для которого $V(s) = 0$. Алгоритм состоит из двух циклично повторяемых этапов: этапа сбора данных и этапа их обработки. На этапе сбора данных при помощи текущего построенного дерева по параметрам среды \textit{I} определяется состояние среды алгоритма обучения с подкреплением \textit{s}. Затем агент \textit{жадно} выбирает действие \textit{a} и сохраняет полученный кортеж \textit{(I, a, I', r)},где \textit{I} -- исходные параметры среды, \textit{a} -- выбранное действие, \textit{I'} -- параметры среды после применения действия \textit{a}, \textit{r} -- награда, полученная агентом. Затем на основе полученного опыта агент обновляет значение \textit{Q(s, a)}. На этапе обработки для каждого сохраненного кортежа вычисляется значение $q(I, a) =  r + \gamma V (s')$~-- значение ожидаемой награды после применения действия \textit{a} к среде с параметрами \textit{I}, где \textit{s'}~-- состояние среды обучения с подкреплением, соответствующее параметрам среды \textit{I'}. Для каждого состояния \textit{s} при помощи критерия разбиения ищется точка разбиения. Если такая точка найдена, то состояние s разбивается на два новых состояния. Множество сохраненных кортежей \textit{(I, a, I', r)} распределяется по новым состояниям в соответствии с разбиением, затем обновляются функция награды и функция переходов.

\begin{algorithm}[h!]
    \caption{Алгоритм \textit{UTree} для непрерывного случая с использованием \textit{Q}-обучения}
    \label{utree_scheme}
    \textbf{Фаза обучения}
    \begin{algorithmic}[1]
        \STATE {По параметрам среды \textit{I} найти соответствующее состояние \textit{s}, являющиеся листом дерева решений.}
        \STATE {Выбрать действие (интервал значений параметра): $a = \argmax\limits_{a'}{Q(s, a')}$.}
        \STATE {Применить выбранное действие к среде, получив награду \textit{r}.}
        \STATE {Сохранить переход $(I, a, I', r)$ в состоянии \textit{s}.}
        \STATE {Обновить значение \textit{Q(s, a)} и $V(s) = \max\limits_a{Q(s, a)}$.}
    \end{algorithmic}
    \textbf{Фаза разбиения}
    \begin{algorithmic}[1]
        \FOR {состояния \textit{s}} 
            \FOR {переход $(I, a, I', r)$ в состоянии \textit{s}}
                \STATE {$q(I, a) = r + \gamma V(s')$}
            \ENDFOR
            \STATE С помощью \textit{критерия разбиения} определить параметр среды и его значение, по которому лучше разделить состояние.
            \IF {найдена точка разбиения}
                \STATE Создать два новых состояния $s_1$ и $s_2$.
                \STATE Распределить переходы состояния \textit{s} по состояниям $s_1$ и $s_2$ в соответствие с точкой разбиения.
                \STATE Рассчитать \textit{Q($s_1$, a)} и \textit{Q($s_2$, a)} по сохраненным переходам.
                \STATE Заменить состояние \textit{s} в дереве решений, на вершину с детьми $s_1$ и $s_2$ и условием выбора, соответствующим точке разбиения.
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsubsection{Критерий типа Колмогорова-Смирнова}
\label{ks_criteria}
В статистическом анализе используют различные критерии однородности для проверки гипотезы о принадлежности двух независимых выборок одному закону распределения. Одним из наиболее используемых непараметрических критериев о проверке однородности двух эмпирических законов распределения является критерий однородности Смирнова.

Эмпирическая функция распределения является приближением теоретической функции распределения, построенное с помощью выборки из него. Пусть $\{X_i\}_{i = 1}^n$ выборка из случайной величины $X$, объема $n$. Эмпирической функцией распределения случайной величины $X$ называется случайная величина $F(x) = \frac{1}{n}\sum\limits_{i = 1}^n{H(x - X_i)}$, где $H$~-- функция Хевисайда. По сути заданная таким образом функция распределения в точке $x$ равна частоте элементов выборки, не превосходящих $x$.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{ks.png}
    \caption{График $F_{a, n}$ и $F_{b, m}$}
    \label{ks}
\end{figure}

Критерий позволяет найти точку, в которой сумма накопленных частот расхождений наибольшая, и оценить достоверность этого расхождения. В качестве нулевой гипотезы $H_0$ принимается, что две исследуемые выборки подчиняются одному закону распределения случайной величины. Для двух независимых выборок $a$ и $b$, объемами $n$ и $m$ соответственно, строятся эмпирические функции распределения $F_{a, n}$ и $F_{b, m}$. Затем считается значение $\sqrt{\frac{nm}{n + m}}D_{n, m}$, где $D_{n, m} = \sup\limits_x|F_{a, n}(x) - F_{b, m}(x)|$. Если рассчитанное значение превышает квантиль распределения Колмогорова $K_{\alpha}$ для заданного уровня значимости $\alpha$, то нулевая гипотеза $H_0$ отвергается.

\subsection{Метод \textit{earpc}}
\label{earpc}
Одним из методов адаптивной настройки параметров ЭА не использующих обучение с подкреплением является метод \textit{earpc}. В данном методе выбор значений параметров происходит независимо друг от друга. В данном метода для выбора значения параметра диапазон допустимых значений делится на два подинтервала во время работы алгоритма.

Схема работы алгоритма представлена на листине~\ref{earpc_scheme}. В ходе работы алгоритма каждому назначению параметров $(v_1, .., v_n)$ сооветствует некоторый функционал качества $q(\textbf(v))$. Выбор новых значений паратров основан на ранее выбранных назначениях параметров. Они разбиваются на два кластера $c_1$ и $c_2$, например с помощью алгоритма \textit{k-means}. Затем для каждого из параметров \textit{v} перебираются подходящие точки разбиения. Можно рассмотреть в качестве точек разбиения значения между двумя соседними точками назначения. Множество назначений разбивается в соответствии с точкой разбиения \textit{s} на множества $p_1$ и $p_2$. Множеству $p_1$ соответствует интервал $[v_{min}, s]$, а множеству $p_2$~-- интервал $(s, v_{max}]$. Обозначим $c_i(p_j)$ -- подмножество $p_j$, соответствующее кластеру $c_i$. Для каждого разбиения по формуле~(\ref{entropy}) считается \textit{энтропия}. Итоговое разбиение выбирается при минимизации энтропии. Для каждого множества $p_i$ считается среднее качество $Q_i$.Случайным образом выбрать интервал значений, при этом вероятность выбора первого пропорциональна $Q_1$, а второго -- $Q_2$. Затем значения параметра \textit{v} случайным образом выбираются из соответствующего множества.

\begin{align}
\label{entropy}
e_{p_1} & = -\frac{|c_1(p_1)|}{|p_1|}ln(\frac{|c_1(p_1)|}{|p_1|}) -\frac{|c_2(p_1)|}{|p_1|}ln(\frac{|c_2(p_1)|}{|p_1|}), \nonumber \\
e_{p_2} & = -\frac{|c_1(p_2)|}{|p_2|}ln(\frac{|c_1(p_2)|}{|p_2|}) -\frac{|c_2(p_2)|}{|p_2|}ln(\frac{|c_2(p_2)|}{|p_2|}), \\
H & = \frac{|p_1|}{|c_1|}e_{p_1} + \frac{|p_2|}{|c_2|}e_{p_2} \nonumber 
\end{align}


\begin{algorithm}[h!]
    \caption{Алгоритм \textit{earpc} в случае деления на два подинтервала.}
    \label{earpc_scheme}
    \begin{algorithmic}[1]
        \STATE {Ранее выбранные назначения параметров $\{(v_1, .., v_n)\}$ разбиваются на два кластера $c_1$ и $c_2$ с помощью алгоритма \textit{k-means}.}
	\FOR {параметр $v_i$}
	    \STATE {Отсортировать назначения параметров по значению \textit{i}-ого}
	    \STATE {$H_{best} \gets \infty$}
	    \FOR {точка разбиения $s = \frac{v_{ij} + v_{i(j+1)}}{2}$}
	      \STATE {Разбить назначения в соответствии с точкой разбиения \textit{s} на множества $p_1$ и $p_2$}
	      \STATE {Расчитать энтропию \textit{H} разбиения по точке \textit{s} по формуле~(\ref{entropy})} 
	      \IF {$H_{best} < H$}
		\STATE {$H_{best} \gets H$}
		\STATE {Запомнить множества $p_1$ и $p_2$}
	      \ENDIF
	    \ENDFOR
	    \STATE {$Q_1 = \frac{1}{|p_1|}\sum\limits_{\textbf{v} \in p_1}{q(\textbf{v})}$, $Q_2 = \frac{1}{|p_2|}\sum\limits_{\textbf{v} \in p_2}{q(\textbf{v})}$}
	    \STATE {Случайным образом выбрать интервал значений, при этом вероятность выбора первого пропорциональна $Q_1$, а второго -- $Q_2$}
	    \STATE {Случайным образом выбрать значение параметра $v_i$ из выбранного интервала}
	\ENDFOR
    \end{algorithmic}
\end{algorithm}

