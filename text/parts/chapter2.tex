\chapter{Разработанные методы настройки параметров ЭА}
\label{proposed_chapter}
\section{Цели}
Таким образом есть метод адаптивной настройки параметров ЭА с помощью обучения с подкреплением, позволяющий адаптивно выделять состояния среды. Однако мнжество действий агента задается априорно. Также есть методы настройки параметров ЭА, позволяющие адаптивно разбивать диапазон допустимых значений параметра.

В данной работе предлагается исследовать эффективность адаптивного выделения множества действий агента засчет разбиения диапазон допустимых значений параметра в ходе работы алгоритма. 

\section{Метод на основе \textit{earpc} и \textit{UTree}}
\label{composing_method}
Данный метод является объединением методов \textit{earpc}~(\ref{earpc}) и \textit{UTree}~(\ref{utree}). В отличие от метода предложенного Карафотиасом~(\ref{karafotias}), в листах дерева решений алгоритма \textit{UTree} содержится метод \textit{earpc}, а не метод \textit{Q}-обучения. В качестве оценки качества выбранного назначения параметров для алгоритма \textit{earpc} используется награда, получаемая агентом. Для алгоритма \textit{UTree} необходимо задать ожидаемую награду $V(s)$ в листе \textit{s}. $V(s) = \sum\limits_{i = 1}^2{\frac{Q_i^2}{Q_1 + Q_2}}$, что соответствует матетматическому ожиданию награды. В самом деле $Q_i$ средняя награда для $i$-ого интервала, а $i$-ый интервал выбирается с вероятностью пропорциональной его средней награде. Также стоит отметить, что при выделении нового состояния среды ожидаемые награды автоматически пересчитываются, поскольку меняется множество переходов в листьях.

\section{Метод на основе распределения наград}
\label{dist_method}

В данной работе предлагается метод адаптивной настройки параметров ЭА с помощью \textit{Q}-обучения с адаптивным выделением множества действий. В процессе работы алгоритма есть лишь одно активное состояние среды, которое характеризуется множеством допустимых действий. Оно является декартовым произведением множеств действий, соответствующих некоторому подинтервалу значений каждого из параметров.И выбор соответствующего назначения параметров происходит согласованно.

Агент выбирает действие на основе алгоритма \textit{Q}-обучения с $\epsilon$-жадной стратегией исследования среды. Если агент имеет мало информации о качестве действий, а именно наибольшая награда малоотличима от средней. То пересматривается текущее разбиение диапазонов значения параметров. Если оно изменилось, то среда меняет активное состояние. При этом за счет изменения разбиения меняется множество допустимых действий агента.

В процессе работы алгоритма сохраняется выбранное назначение параметров и полученная, за это назначение награда. Разбиение диапазона значений для каждого из настраиваемых параметров просходит с помощью критерия Колмогорова-Смирнова аналогично алгоритму \textit{UTree}. В отличие от алгоритма \textit{UTree} дерево решений не строится, так как параметр разбиения лишь один. Задается лишь глубина разбиения. Схема формирования множества действий представлена на листинге~\ref{dist_scheme}.


\begin{algorithm}[h!]
    \caption{Алгоритм разбиения в \textit{dist} методе глубины 2.}
    \label{dist_scheme}
    \begin{algorithmic}[1]
	\REQUIRE  
	  \textit{V} --- множество назначений параметров;
        \FOR {параметр \textit{v}}
	  \STATE {Разбиение $P \gets {}$}
	  \STATE {Отсортировать множество назначений по параметру \textit{v}}
	  \STATE {С помощью критерия Колмогорова-Смирнова найти точку разбиения \textit{s} множества \textit{V}}
	  \IF {Точка разбиения \textit{s} не найдена}
	    \STATE {$P \gets \{[v_{min}, v_{max}]\}$}
	  \ELSE
	    \STATE {Разбить множество \textit{V} на \textit{L} и \textit{R} в соответствии с \textit{s}}
	    \STATE {Найти точку разбиения $s_l$ для множества \textit{L}}
	    \STATE {Найти точку разбиения $s_r$ для множества \textit{R}}
	    \IF {Точки разбиения $s_l$ и $s_r$ не найдены}
	      \STATE {$P \gets \{[v_{min}, s], (s, v_{max}]\}$}
	    \ELSIF {Точка разбиения $s_l$ не найдена}
	      \STATE {$P \gets \{[v_{min}, s], (s, s_r], (s_r, v_{max}]\}$}
	     \ELSIF {Точка разбиения $s_r$ не найдена}
	      \STATE {$P \gets \{[v_{min}, s_l], (s_l, s], (s, v_{max}]\}$}
	     \ELSE
	      \STATE {$P \gets \{[v_{min}, s_l], (s_l, s], (s, s_r], (s_r, v_{max}]\}$}
	    \ENDIF
	  \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}