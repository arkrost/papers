\chapter{Разработанные методы настройки параметров ЭА}
\label{proposed_chapter}
\section{Цель работы}
Существует метод адаптивной настройки параметров ЭА с помощью обучения с подкреплением, позволяющий адаптивно выделять состояния среды. Множество действий агента определяется разбиением диапазона допустимых значений параметра, которое задается до начала выполнения алгоритма. Также существуют методы настройки параметров ЭА, позволяющие адаптивно разбивать диапазон допустимых значений параметра.

В данной работе предлагается исследовать эффективность адаптивного выделения множества действий агента за счет разбиения диапазона допустимых значений параметра в ходе работы алгоритма. Целью исследований являлась разработка метода адаптивной настройки параметров эволюционного алгоритма с помощью обучения с подкреплением. Предлагаемый алгоритм на основе обучения с подкреплением должен формировать множество действий агента во время работы, адаптивно разбивая диапазон допустимых значений параметра.

\section{Метод на основе \textit{earpc} и \textit{UTree}}
\label{composing_method}
Данный метод является объединением методов \textit{earpc}~(\ref{earpc}) и \textit{UTree}~(\ref{utree}). В отличие от метода, предложенного Karafotias et al.~(\ref{karafotias}), выбор значений параметров происходит с помощью на основе алгоритма \textit{earpc}. В процессе работы по наблюдаемым характеристикам ЭАстроится дерево решений \textit{UTree}. Алгоритм \textit{earpc} выбирает значения параметров на основе, сохраненных в листе переходов $(I, a, I', r)$. В качестве оценки качества выбранного назначения параметров для алгоритма \textit{earpc} используется награда, получаемая агентом. Таким образом для определения значения параметров, необходимо по характеристикам среды найти лист дерева \textit{UTree}. Затем с помощью алгоритма \textit{earpc} выбрать значения параметров, по переходам хранящимся в листе.

При построении дерева \textit{UTree} необходимо определить как разбить лист на два состояния. Чтобы применить критерий разбиения для каждого перехода $(I, a, I', r)$, сохраненного в листе, считается значение $q(I, a) = r + \gamma V(s')$. При этом необходимо посчитать ожидаемую награду $V(s')$ в листе \textit{s'}. Предлагается посчитать математическое ожидание награды в листе \textit{s'}. Алгоритм \textit{earpc} разбивает диапазон значений параметра на два подинтервала, один из которых выбирается с вероятностью пропорциональной средней награде на подинтервале. Значит, $V(s) = \sum\limits_{i = 1}^2{\frac{Q_i^2}{Q_1 + Q_2}}$, где $Q_1$ и $Q_2$ среднее значение награды на первом и втором подинтервале соответственно. 

Кроме того, после выделения нового состояния среды необходимо пересчить ожидаемые награды для получившихся состояний. В предлагаемом методе это происходит автоматически, поскольку при выделении нового состояния в соответствии с алгоритмом \textit{UTree} перераспределяется множество переходов.

\section{Метод на основе распределения наград}
\label{dist_method}

Также в данной работе предлагается метод адаптивной настройки параметров ЭА с помощью \textit{Q}-обучения с адаптивным выделением множества действий. В процессе работы алгоритма есть лишь одно активное состояние среды, которое характеризуется множеством допустимых действий. Оно является декартовым произведением множеств действий, соответствующих некоторому подинтервалу значений каждого из параметров. И выбор соответствующего назначения параметров происходит согласованно.

Агент выбирает действие на основе алгоритма \textit{Q}-обучения с $\epsilon$-жадной стратегией исследования среды. Если агент имеет мало информации о качестве действий, а именно наибольшая награда малоотличима от средней. То пересматривается текущее разбиение диапазонов значения параметров. Если оно изменилось, то среда меняет активное состояние. При этом за счет изменения разбиения меняется множество допустимых действий агента.

В процессе работы алгоритма сохраняется выбранное назначение параметров и полученная, за это назначение награда. Разбиение диапазона значений для каждого из настраиваемых параметров просходит с помощью критерия Колмогорова-Смирнова аналогично алгоритму \textit{UTree}. В отличие от алгоритма \textit{UTree} дерево решений не строится, так как параметр разбиения лишь один. Задается лишь глубина разбиения. Схема формирования множества действий представлена на листинге~\ref{dist_scheme}.


\begin{algorithm}[h!]
    \caption{Алгоритм разбиения в \textit{dist} методе глубины 2.}
    \label{dist_scheme}
    \begin{algorithmic}[1]
	\REQUIRE  
	  \textit{V} --- множество назначений параметров;
        \FOR {параметр \textit{v}}
	  \STATE {Разбиение $P \gets \emptyset$}
	  \STATE {Отсортировать множество назначений по параметру \textit{v}}
	  \STATE {С помощью критерия Колмогорова-Смирнова найти точку разбиения \textit{s} множества \textit{V}}
	  \IF {Точка разбиения \textit{s} не найдена}
	    \STATE {$P \gets \{[v_{min}, v_{max}]\}$}
	  \ELSE
	    \STATE {Разбить множество \textit{V} на \textit{L} и \textit{R} в соответствии с \textit{s}}
	    \STATE {Найти точку разбиения $s_l$ для множества \textit{L}}
	    \STATE {Найти точку разбиения $s_r$ для множества \textit{R}}
	    \IF {Точки разбиения $s_l$ и $s_r$ не найдены}
	      \STATE {$P \gets \{[v_{min}, s], (s, v_{max}]\}$}
	    \ELSIF {Точка разбиения $s_l$ не найдена}
	      \STATE {$P \gets \{[v_{min}, s], (s, s_r], (s_r, v_{max}]\}$}
	     \ELSIF {Точка разбиения $s_r$ не найдена}
	      \STATE {$P \gets \{[v_{min}, s_l], (s_l, s], (s, v_{max}]\}$}
	     \ELSE
	      \STATE {$P \gets \{[v_{min}, s_l], (s_l, s], (s, s_r], (s_r, v_{max}]\}$}
	    \ENDIF
	  \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}